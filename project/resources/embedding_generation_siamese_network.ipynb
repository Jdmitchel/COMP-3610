{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Siamese Networks for Embedding Generation\n",
        "\n",
        "This notebook demonstrates how to build and train a Siamese Network to generate vector embeddings. Siamese Networks consist of two identical subnetworks that merge at their outputs, designed to learn embeddings so that similar items are closer in the embedding space compared to dissimilar items. This approach is particularly useful for tasks such as face recognition, signature verification, and in general, for learning a similarity metric between two inputs.\n",
        "\n",
        "## What are Siamese Networks?\n",
        "\n",
        "Siamese Networks are a type of neural network architecture that contain two or more identical subnetworks meant to process two separate inputs. The idea is that by comparing these inputs as they pass through the network, the model learns to distinguish between them based on their similarity. This training method allows the network to effectively generate embeddings â€“ representations of input data in a lower-dimensional space.\n",
        "\n",
        "## Why Generate Embeddings?\n",
        "\n",
        "Generating embeddings can significantly enhance performance on tasks like classification, recommendation, and clustering by representing complex data (like images or text) in a form that highlights their relative similarities or differences. Embeddings make it possible to perform these tasks even with limited data, as they capture the essence of the data's features in a compressed form.\n",
        "\n",
        "## How to Implement a Siamese Network\n",
        "\n",
        "This notebook covers the following steps:\n",
        "\n",
        "1. **Environment Setup**: Installation of necessary libraries (TensorFlow).\n",
        "2. **Siamese Network Architecture**: Designing the architecture using TensorFlow/Keras.\n",
        "3. **Data Preparation**: Preparing pairs of data for training.\n",
        "4. **Model Training**: Training the Siamese Network to generate embeddings.\n",
        "5. **Embedding Generation**: Using the trained network to generate vector embeddings for new data.\n",
        "\n",
        "For more detailed information on Siamese Networks and embedding generation, the following resources might be helpful:\n",
        "\n",
        "- Siamese Networks: [Understanding Siamese Networks](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n",
        "- Vector Embeddings: [Learning Embeddings](https://towardsdatascience.com/understanding-feature-vectors-extraction-cdff276b9b65)\n",
        "- Embedding Generation: [Generating Embeddings with Neural Networks](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n",
        "\n",
        "By following this notebook, you'll gain a practical understanding of how to leverage Siamese Networks for embedding generation, providing a foundation for further exploration into more complex architectures and applications.\n"
      ],
      "metadata": {
        "id": "jvWzrw_md7Sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me_FnhGCYhuZ",
        "outputId": "a75e7eaf-5478-4409-c145-95fb5f1070a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas numpy tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import backend as K\n",
        "import random"
      ],
      "metadata": {
        "id": "bpTVUyB7YqCG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pairs(dataset, num_classes):\n",
        "    (train_digits, train_labels), (test_digits, test_labels) = dataset.load_data()\n",
        "    # Normalize image vectors\n",
        "    train_digits = train_digits.astype('float32') / 255.0\n",
        "    test_digits = test_digits.astype('float32') / 255.0\n",
        "\n",
        "    # Expand dimensions to add channel information\n",
        "    train_digits = np.expand_dims(train_digits, axis=-1)\n",
        "    test_digits = np.expand_dims(test_digits, axis=-1)\n",
        "\n",
        "    # Create pairs\n",
        "    digit_indices = [np.where(train_labels == i)[0] for i in range(num_classes)]\n",
        "    tr_pairs, tr_y = create_pairs(train_digits, digit_indices)\n",
        "\n",
        "    digit_indices = [np.where(test_labels == i)[0] for i in range(num_classes)]\n",
        "    te_pairs, te_y = create_pairs(test_digits, digit_indices)\n",
        "\n",
        "    return (tr_pairs, tr_y), (te_pairs, te_y)\n",
        "\n",
        "def create_pairs(x, digit_indices):\n",
        "    \"\"\"Positive and negative pair creation.\n",
        "    Alternates between positive and negative pairs.\"\"\"\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
        "\n",
        "    for d in range(10):\n",
        "        for i in range(n):\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            inc = random.randrange(1, 10)\n",
        "            dn = (d + inc) % 10\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[x[z1], x[z2]]]\n",
        "            labels += [1, 0]\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "# Load the MNIST dataset\n",
        "num_classes = 10\n",
        "(tr_pairs, tr_y), (te_pairs, te_y) = load_pairs(mnist, num_classes)"
      ],
      "metadata": {
        "id": "FCRmbTFPYxSM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_base_network(input_shape):\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(input)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    return Model(input, x)\n",
        "\n",
        "def euclidean_distance(vects):\n",
        "    x, y = vects\n",
        "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
        "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
        "\n",
        "def eucl_dist_output_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0], 1)\n",
        "\n",
        "# Network definition\n",
        "input_shape = tr_pairs[:, 0].shape[1:]\n",
        "base_network = initialize_base_network(input_shape)\n",
        "\n",
        "input_a = Input(shape=input_shape)\n",
        "input_b = Input(shape=input_shape)\n",
        "\n",
        "# Because we re-use the same instance `base_network`,\n",
        "# the weights of the network will be shared across the two branches\n",
        "processed_a = base_network(input_a)\n",
        "processed_b = base_network(input_b)\n",
        "\n",
        "distance = Lambda(euclidean_distance, output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
        "\n",
        "model = Model([input_a, input_b], distance)"
      ],
      "metadata": {
        "id": "1oSYcVhPY1sF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftB9y3eSZHYw",
        "outputId": "7725e0a8-68e6-4bec-b672-8589429af751"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "847/847 [==============================] - 162s 190ms/step - loss: 0.7308 - accuracy: 0.4703 - val_loss: 0.7339 - val_accuracy: 0.4642\n",
            "Epoch 2/10\n",
            "847/847 [==============================] - 152s 179ms/step - loss: 0.7146 - accuracy: 0.4899 - val_loss: 0.7278 - val_accuracy: 0.4681\n",
            "Epoch 3/10\n",
            "847/847 [==============================] - 156s 184ms/step - loss: 0.7056 - accuracy: 0.5084 - val_loss: 0.7246 - val_accuracy: 0.4668\n",
            "Epoch 4/10\n",
            "847/847 [==============================] - 150s 177ms/step - loss: 0.6989 - accuracy: 0.5200 - val_loss: 0.7239 - val_accuracy: 0.4612\n",
            "Epoch 5/10\n",
            "847/847 [==============================] - 150s 177ms/step - loss: 0.6936 - accuracy: 0.5344 - val_loss: 0.7228 - val_accuracy: 0.4643\n",
            "Epoch 6/10\n",
            "847/847 [==============================] - 151s 178ms/step - loss: 0.6896 - accuracy: 0.5436 - val_loss: 0.7234 - val_accuracy: 0.4614\n",
            "Epoch 7/10\n",
            "847/847 [==============================] - 151s 178ms/step - loss: 0.6872 - accuracy: 0.5485 - val_loss: 0.7248 - val_accuracy: 0.4694\n",
            "Epoch 8/10\n",
            "847/847 [==============================] - 151s 178ms/step - loss: 0.6828 - accuracy: 0.5596 - val_loss: 0.7222 - val_accuracy: 0.4701\n",
            "Epoch 9/10\n",
            "847/847 [==============================] - 162s 192ms/step - loss: 0.6806 - accuracy: 0.5639 - val_loss: 0.7210 - val_accuracy: 0.4727\n",
            "Epoch 10/10\n",
            "847/847 [==============================] - 150s 178ms/step - loss: 0.6784 - accuracy: 0.5693 - val_loss: 0.7234 - val_accuracy: 0.4702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just to get something to test the newly trained network on quickly we reload data from MNIST\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Select an image for testing\n",
        "# For simplicity, we're taking the first image from the test set\n",
        "new_image_array = test_images[0]\n",
        "\n",
        "# Siamese networks expect input data to have a specific shape, often including the channel dimension.\n",
        "# MNIST images are 28x28 and grayscale, so we need to reshape them to 28x28x1.\n",
        "new_image_array = np.expand_dims(new_image_array, axis=-1)\n",
        "\n",
        "# Normalize the image\n",
        "new_image_array = new_image_array.astype('float32') / 255.0\n",
        "\n",
        "new_image_embedding = base_network.predict(np.array([new_image_array]))\n",
        "\n",
        "print(\"Generated Embedding:\", new_image_embedding)\n",
        "new_image_embedding.shape # A vector of length 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "teFNVfYcZLSM",
        "outputId": "fe9f5680-26cc-4321-9e50-5566ebefcd01"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 28, 28, 1) dtype=float32>]\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-46ee99b63496>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnew_image_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_image_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnew_image_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_image_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generated Embedding:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_image_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 219, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_1\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 28, 28, 1) dtype=float32>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('siamese_network.h5')  # saves the entire model to a HDF5 file\n",
        "model.save_weights('siamese_network_weights.h5')  # saves just the model weights\n",
        "base_network.save('base_network.h5') # saves just base network, for actually generating embedding rather than the full siamese architecture"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u1t8ngvZ38c",
        "outputId": "b5894bdc-592f-40fe-8ff1-7d35ae749633"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('siamese_network.h5') # load the entire model\n",
        "model.load_weights('siamese_network_weights.h5') # If you've only saved the weights and want to load them, you'll need to define the model architecture first and then load the weights\n",
        "loaded_base_network = load_model('base_network.h5') # loading the base network"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2NmBBZCiU7u",
        "outputId": "421c89bf-b233-4c74-e218-d433c5bc4012"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfL43rFdjdC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}